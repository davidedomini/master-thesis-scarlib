%%%%
% Consiglio la visione dei seguenti tutorial:
% - https://www.youtube.com/watch?v=ihxSUsJB_14
% - https://www.youtube.com/watch?v=XTFWaV55uDo
%%%%
\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\newcommand{\fonte}[1]{{\color{gray} \small \hypersetup{citecolor=gray} Source: #1}}

%\newcommand{\thesislang}{italian} % decommentare in caso di tesi in italiano
\newcommand{\thesislang}{english} % commentare in caso di tesi in italiano
\usepackage{thesis-style}
% version
\newcommand{\versionmajor}{0}
\newcommand{\versionminor}{1}
\newcommand{\versionpatch}{2}
\newcommand{\version}{\versionmajor.\versionminor.\versionpatch}
\typeout{Document version: \version}

\begin{document}
	
\frontmatter

\input{front.tex}

\begin{abstract}	
Max 2000 characters, strict.
\end{abstract}

\begin{dedication} 
Optional. Max a few lines.
\end{dedication}

\begin{acknowledgements}
Optional. Max 1 page.
\end{acknowledgements}

%----------------------------------------------------------------------------------------
\tableofcontents   
\listoffigures     
\lstlistoflistings 
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{\introductionname}
\label{chap:introduction}
%----------------------------------------------------------------------------------------


\paragraph{Thesis motivation} % TODO --- Riformulare e approfondire meglio

Significant technological advancements have paved the way for the emergence of a field known as \emph{Collective Computing} 
    \cite{abowd2016beyond}, with \emph{Cyber-Physical Swarms (CPSW)} \cite{schranz2021swarm} as a noteworthy branch within it.
    The latter consist of myriad devices that interact with the environment and exchange information among themselves. 
    A crucial aspect of these systems is that a more complex collective behavior emerges from the interaction between 
    individual agents that leads to the resolution of various tasks.
    Among all aspects related to CPSW, our focus lies on properties like \emph{collective intelligence} \cite{tumer2004survey} 
    and \emph{self-organization} \cite{schmeck2011organic}. This stems from the applications of these systems, leading us to 
    concentrate on their collective behavior to express autonomy, adaptability, and coordination of the devices 
    that are part of them.

This progress has been driven by research in various related fields such as: multi-agent systems \cite{dorri2018multi},
     coordination \cite{yang2022overview}, distributed artificial intelligence \cite{bond2014readings}, and many others. 
     Additionally, it has a profound impact on a wide range of applied domains, including: smart cities \cite{zedadra2019swarm}, 
     swarm robotics \cite{brambilla2013swarm}, large-scale IoT systems \cite{uslu2023role}, and more.

A crucial aspect to consider in CPSW is how individual devices are programmed and achieve coordination to perform assigned tasks. 
Novel approaches -- like \emph{aggregate computing} \cite{viroli2018field} -- have focused on manually developing
controllers from a global perspective. However, this approach has some drawbacks: it is highly challenging to write satisfactory 
and efficient programs for complex tasks, they may be error-prone and lack of generality.

On the other hand, there exists approaches that leverage various artificial intelligence (AI) techniques, 
such as \emph{Multi-Agent Reinforcement Learning} (MARL) \cite{busoniu2008comprehensive},
to enable devices to learn directly from experience and/or data. These approaches also present several challenges, including: non-stationarity 
\cite{hernandez2017survey}, communication and scalability.

%
\paragraph{Thesis objectives}

Starting from what was seen in the previous paragraph, the goal of this thesis is to lay the foundation for a hybrid approach 
    that can succeed in exploiting the potential of both macro-programming, in particular \emph{aggregate computing} is taken as a 
    reference, and AI approach.
    In order to achieve this goal, it is necessary to develop a toolchain that allows these systems to be developed in an agile,
    fast and reusable way. 
    Scarlib, whose development has already started in \cite{scarlib}, is the tool that for us forms the basis of this toolchain.
    Its main purpose is to integrate the \emph{ScaFi} \cite{casadei2022scafi} (an implementation of aggregate computing) 
    and \emph{Alchemist} \cite{pianini2013chemical} (a bio-chemical based simulator) tools with \emph{Reinforcement Learning} 
    to help develop experiments in \emph{simulated} environments with \emph{offline learning}.

%
\paragraph{Thesis Structure} 


%----------------------------------------------------------------------------------------
\chapter{Background}
\label{chap:background}
%----------------------------------------------------------------------------------------

\section{Cyber-Physical Swarms}
%
\section{Aggregate Computing}

The advent of \emph{Collective Computing} and the proliferation of interconnected devices have given rise to novel 
    paradigms that aim to address the challenges posed by the distributed nature of computing
    systems. One such paradigm that has gained significant attention in recent years is 
    \emph{aggregate computing (AC)} [8].

AC is well grounded on \emph{field calculus}, and therefore adopts a model where the perspective is at a global level: 
    a group of devices is seen as a global entity (i.e., the \emph{aggregate system}) that works at asynchronous 
    rounds and exchanges messages with neighbours. 
    A round is composed of three phases:
    i) \emph{Context Building}, each node collects information from the 
        neighborhood and sensors,
    ii) \emph{Program Execution}, each node executes the aggregate program on the local context, and
    iii) \emph{Export Sharing}, each node shares the export with the neighborhood.

Interactions within the aggregate system are seen as a flow of information propagating through the 
    collective of devices, rather than as local interactions of individual devices with their
    peers and the environment. This approach offers a number of advantages:
    i) the program can be defined in a composable and declarative manner, 
    ii) it promotes the reuse of behaviours, and 
    iii) the programmer is relieved from concerns regarding low-level aspects (e.g., failures, distribution, communication and more), 
    as these are automatically handled by the middleware.
%Considering the interactions within this system as a flow of information 
%    propagating through a collective of devices, rather than as local interactions between devices and the 
%    environment, offers significant advantages: i) the program can be defined in a composable and declarative
%    manner, ii) it promotes the reuse of behaviours, and iii) the programmer is relieved from concerns regarding 
%    low-level aspects (e.g., failures, distribution, communication and more), as these are automatically handled 
%    by the middleware. %riformula tipo "le interazioni sono viste come... e questo porta i seguenti vantaggi..."

In aggregate computing, information is represented by a distributed data structure known as computational field [50, 26],
    which is an abstraction of space-time values where each device is mapped to a computational value.
    The manipulation of these fields is derived from the Field Calculus [48], a computational model in which collective 
    behaviours are expressed as algorithms that are the composition of computational fields.

In recent times, several implementations of aggregate computing have been developed, and one particularly interesting 
    implementation is ScaFi [13]. ScaFi is a Scala-based platform that offers the following features: 
    i) a domain specific language (DSL) for specifying aggregate computation, 
    ii) asimulation environment (through the Alchemist simulator [34]), 
    iii) a middleware for executing and deploying aggregate programs, and 
    iv) reusable library functionalities that serve as building blocks for constructing new aggregate programs. 
    For example, the gradients abstraction that provides gradient functions [47, 7] used for the ongoing computation, 
    across spatio-temporal dimension, of the self-healing field (i.e., a field able to self-adjust in case of changes 
    in devices topology) that determines the minimum distances of individual nodes from a specified set of source 
    nodes [13].

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/channel.png}
        \caption{Channel example}
        \label{fig:channel}
    \end{figure}

      \lstinputlisting[float,language=Scala,label={lst:channel-code}]{listings/Channel.scala}

%
\section{Reinforcement Learning}
%

``\emph{Reinforcement Learning (RL)} is the science of decision making. It is about learning the optimal behavior 
    in a environment to obtain maximum reward" 
    \footnote{\url{https://www.synopsys.com/ai/what-is-reinforcement-learning.html}}.
    RL is a general framework, other than supervised and unsupervised learning, in which an \emph{agent} learns 
    to behave within an \emph{environment} by performing some \emph{actions} and seeing the result they produce.
    It is inspired by how humans and animals learn through the system of rewards and punishments: for each good action
    the environment provides to the agent a positive reward, instead, for each bad action the agent gets a negative 
    reward (also called penalty).

Formally, a RL problem can be formulated as following \cite{RLSurvey}:
    \begin{itemize}
        \item Discrete time steps $t=0, 1, 2, ...$;
        \item A discrete set of environment states $\mathcal{S}$;
        \item A discrete set of agent actions $\mathcal{A}$;
        \item A reinforcement signal;
        \item A probabilistic policy $\pi$, that is a mapping function from states to actions;
        \item The goal of the agent is to learn the optimal policy $\pi^*$ in order to maximize 
                some long-run measure of reinforcement (e.g., the Infinite Horizon Discounted Model \cite{RLSurvey}).
    \end{itemize}
    First, at time $t$, the agent observes the state of the environment $s_t \in \mathcal{S}$
        and chooses an action $a_t \in \mathcal{A}$ using the actual policy $\pi_t$. 
        Thereafter, the environment: takes in the action $a_t$, emits the new state $s_{t+1} \in \mathcal{S}$ 
        and returns the scalar reward $r_{t+1}$.
        Finally, the agent, based on the reward obtained updates its knwoledge.

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/rl.png}
        \caption{Reinforcement learning schema}
        \fonte{\url{https://it.mathworks.com/discovery/reinforcement-learning.html}}
        \label{fig:rl_schema}
    \end{figure}

    In order to find the optimal policy $\pi^*$, the agent tries to maximize the expected cumulative reward.
        Since the environment is stochastic (i.e., the same action performed in the same state could lead to different 
        results over time) the more you look into the future the more the outcome could diverge.
        For this reason, it is common to use a model that takes less account of rewards that are far away in time 
        than those that are close in time:
        $$R_t = \sum_{i=t}^{\infty} \gamma^{i-t} \cdot R_{\pi(s_i)}(s_i, s_{i+1}) $$
        This model is called \emph{Infinite Horizon Discounted Model}, the key aspect is the hyper-parameter $\gamma$.
        It is a scalar weight in the range $[0;1]$, in this way, the further away the reward is in time, the smaller its weight.
\paragraph{Markov Decision Process} 


\paragraph{Exploration-exploitation dilemma}
The \emph{exploration-exploitation dilemma} is a problem that comes from the definition of the RL process.
    In order to increase its knowledge and build an optimal policy, the agent needs to \emph{explore} the environment 
    in the hope of finding better actions. After some exploration, the agent might have found a set of 
    apparently rewarding actions, but, how can the agent be sure that the found actions are actually the best? 
    When should the agent continue to explore or else, when should it just \emph{exploit} its existing knwoledge?

Several exploration strategies have been proposed in the literature to solve this problem, the simplest is the
    \emph{$\epsilon$-greedy} strategy. The agent \emph{randomly explore} the environment with probability $\epsilon$
    while \emph{exploit} the current optimal action with probability $1-\epsilon$.

    $$
    \pi(s)=
    \begin{cases}
        \pi^*(s) & \text{with probability $1-\epsilon$} \\
        \text{\emph{random action}} & \text{with probability $\epsilon$}\\
    \end{cases} 
    $$ 

    Usually, at the beginning of the learning process $\epsilon$ starts near to $1$ (i.e., more exploration) and then decreases
    to $0$ as the agent learns more and more about the environment. 

\paragraph{Main approaches} %Policy vs value based

\begin{figure*}[t]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/policy-based-rl.png}
        \caption{Policy based rl}
        \label{fig:policy-based-rl}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/value-based-rl.png}
        \caption{Value based rl}
        \label{fig:value-based-rl}
    \end{subfigure}
\caption{Policy and value based algorithms visual comparison}\vspace{-10pt}
\end{figure*}

\paragraph{Q-Learning}

\paragraph{Deep Reinforcement Learning}

\begin{figure*}[t]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/q-learning.pdf}
        \caption{Q-Learning}
        \label{fig:ql}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deepQL.pdf}
        \caption{Deep Q-Learning}
        \label{fig:dqn}
    \end{subfigure}
\caption{Q-Learning and Deep Q-Learning visual comparison}\vspace{-10pt}
\end{figure*}

\section{Multi-Agent Reinforcement Learning}

%----------------------------------------------------------------------------------------
\chapter{Requirements} 
\label{chap:requirements}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
\chapter{Design} 
\label{chap:design}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
\chapter{Implementation} 
\label{chap:implementation}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
\chapter{Validation} % possible chapter for Projects
\label{chap:validation}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
\chapter{\conclusionsname}
\label{chap:conclusions}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\nocite{*} % uncomment this to show all the reference in the .bib file
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}